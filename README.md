# MachineLearningProject
Machine Learning (CS4641) - Georgia Institute of Technology Project consist of 5 collaborative students

Team: Bajhat Abugharibeh, Hasan Naseer, Muhammed Abdulbari, Robert White, Luke LaScala
 
With all the technological advancements today, we see the language barriers between all kinds of people being continuously diminished. However, we want to specifically dedicate this project to contribute to the improvement of the means of communication of the American Deaf community. The American Sign Language (ASL) is a language expressed by the hands and face. This language is used to communicate with and between hearing impaired individuals. 
 
 
The majority of the American population has difficulty communicating with hearing impaired individuals aside from the use of text communication. As most people don’t know ASL (including most of us), thus they are slow or unable even to decipher ASL communication. We believe that there is much room for technology to improve these barriers of communication.
The initial focus of our project will be to classify images/ still sign language into English letters. However, we are also considering analyzing videos to classify ASL signs that contain movement. Although, we acknowledge the fact that the latter may be very complicated therefore we might or might not decide to implement it.


The majority of the algorithms that will be used in this project fall under the category of supervised learning. Specifically, we may use tree based classification models as well as support vector machines to classify the images/videos into the ASL letters [1]. This would also involve using neural networks and deep learning pipelines to build the model. Also, it’s worthy to note that we will be classifying only 24 out of the 26 letters in the alphabet as letters z and j involve movements. We are also planning to apply unsupervised machine learning algorithms to the images for dimensionality reduction and for improving the overall performance of our model [3]. We will be using the MNIST dataset that contains 34627 images of signs and their corresponding labels [2].


This project has important applications for both hearing impaired and non-hearing impaired individuals alike. This project could enable quick and efficient communication between the above groups. We plan to successfully classify ASL signs from images. Given that our team achieves a high accuracy rate on most ASL signs, we can make communication easier amongst groups communicating through ASL. We aim to make this project a foundation for larger scale translational applications between ASL and English language.


There is a great deal of software and technology that are being developed to better accommodate individuals with hearing impairments. Even more recently, there has been a trend of image recognition softwares being used by the general public and finding their way into mainstream media. This ASL transcriber will be our introduction to understanding and being able to develop such programs. 
	

References

[1] Julichitai. (2021, February 19). ASL alphabet classification using PyTorch. Retrieved February 28, 2021, from https://www.kaggle.com/julichitai/asl-alphabet-classification-using-pytorch 

[2] Tecperson. (2017, October 20). Sign language mnist. Retrieved March 01, 2021, from https://www.kaggle.com/datamunge/sign-language-mnist 

[3] Sign language recognition using python and opencv. (2020, September 15). Retrieved March 01, 2021, from https://data-flair.training/blogs/sign-language-recognition-python-ml-opencv/ 
